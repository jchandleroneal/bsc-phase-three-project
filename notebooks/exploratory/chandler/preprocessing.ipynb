{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, auc, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('../../../data/EDA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarifying important predictor values (x) for the target value (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the target (y) values from x and specific origin\n",
    "features = df.drop(['ref', 'company_manufacturer', 'company_location', 'review_date', 'country_of_bean_origin', 'specific_bean_origin_or_bar_name', 'cocoa_percent','ingredients', 'most_memorable_characteristics', 'rating', 'rating_class'], axis =1)\n",
    "\n",
    "X = features\n",
    "y = df.rating_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into three subsets of training and validation data for the future models.\n",
    "    Two train test splits create three subsets of the original dataset which allows for the training data to not be bled into the test data - this reduced model's bias towards the pre-existing testing data, thus assuring maximum performance on future test sets to which the model has never been exposed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train test split for test set (subsets 1/3)\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=.15, random_state=42)\n",
    "\n",
    "\n",
    "#performing a train test split for train and validation set (subsets - 3/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=.15, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1896, 8), (1896,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to ensure the shape of the columns and rows are still the same for the X and y values after the train test split.\n",
    "X_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Replacing any existing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingredients have 88 rows that have no imputs.\n",
    "X_val.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the empty value replacement will be done using the most frequent fill strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_val_imputed = pd.DataFrame(imputer.fit_transform(X_val),columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensuring that the missing values from ingredients are now filled in with the most frequent occuring value\n",
    "\n",
    "X_val_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating groups into numeric and catagorical data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at the groups data types to ensure that they are separated correctly\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on training categorical data\n",
    "ohe.fit(X_train)\n",
    "X_train_encoded = ohe.transform(X_train)\n",
    "X_val_encoded = ohe.transform(X_val)\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=ohe.get_feature_names())\n",
    "X_val_encoded_df= pd.DataFrame(X_val_encoded, columns=ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the encoded and origin splitting together\n",
    "X_train_df = pd.concat([X_train, X_train_encoded_df], axis=1)\n",
    "X_val_df = pd.concat([X_val, X_val_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the bins \n",
    "X_train_df = X_train_df.drop(['review_date_bin','bean_origin_bins','comp_manufact_bin','ingredient_list','cocoa_bucket'], axis=1)\n",
    "X_val_df = X_val_df.drop(['review_date_bin','bean_origin_bins','comp_manufact_bin','ingredient_list','cocoa_bucket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../data/feature_engineering.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
